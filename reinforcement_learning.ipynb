{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reinforcement learning email problem\n",
    "\n",
    "suppose i want to send out an email campaign, but i don't know how many times to send an email to a person or how to space them out.\n",
    "\n",
    "we can assume that there is a point where sending more emails will not increase the probability of the person clicking on the email, and it can even have a negative effect.\n",
    "\n",
    "\n",
    "therefore, we want to maximize the expected reward of the person clicking on the email, while minimizing the number of emails sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example response probabilities for first user:\n",
      "Base response rate: 0.354\n",
      "After 1 email: 0.077\n",
      "After 5 emails: 0.036\n",
      "After 10 emails: 0.014\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create 100 users with different response patterns\n",
    "n_users = 100\n",
    "\n",
    "# Generate user data\n",
    "users = pd.DataFrame({\n",
    "    'user_id': range(n_users),\n",
    "    # Some users are more likely to respond to emails than others\n",
    "    'base_response_rate': np.random.beta(2, 5, n_users),  \n",
    "    # How quickly users get annoyed with frequent emails (lower = gets annoyed faster)\n",
    "    'patience_factor': np.random.uniform(0.5, 0.9, n_users),\n",
    "    # How long until response probability starts declining\n",
    "    'optimal_gap_days': np.random.randint(3, 14, n_users)\n",
    "})\n",
    "\n",
    "# Function to simulate email response probability\n",
    "def get_response_probability(user_row, n_previous_emails, days_since_last):\n",
    "    base_rate = user_row['base_response_rate']\n",
    "    patience = user_row['patience_factor']\n",
    "    optimal_gap = user_row['optimal_gap_days']\n",
    "    \n",
    "    # Decay based on number of previous emails\n",
    "    email_fatigue = patience ** n_previous_emails\n",
    "    \n",
    "    # Penalty for suboptimal timing\n",
    "    timing_factor = np.exp(-abs(days_since_last - optimal_gap) / optimal_gap)\n",
    "    \n",
    "    return base_rate * email_fatigue * timing_factor\n",
    "\n",
    "# Generate some example probabilities\n",
    "example_user = users.iloc[0]\n",
    "print(\"\\nExample response probabilities for first user:\")\n",
    "print(f\"Base response rate: {example_user['base_response_rate']:.3f}\")\n",
    "print(f\"After 1 email: {get_response_probability(example_user, 1, 7):.3f}\")\n",
    "print(f\"After 5 emails: {get_response_probability(example_user, 5, 7):.3f}\")\n",
    "print(f\"After 10 emails: {get_response_probability(example_user, 10, 7):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of learned vs actual optimal gaps:\n",
      "   user_id  learned_gap  actual_gap\n",
      "0        0            0           3\n",
      "1        1            9           7\n",
      "2        2            4          11\n",
      "3        3            1           3\n",
      "4        4            4           5\n"
     ]
    }
   ],
   "source": [
    "# Q-learning parameters\n",
    "n_episodes = 1000\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.95\n",
    "epsilon = 0.1  # For epsilon-greedy exploration\n",
    "\n",
    "# Initialize Q-table: state = (user_id, days_since_last), action = wait/send\n",
    "max_days_window = 30  # Maximum days to track since last email\n",
    "n_actions = 2  # 0: wait, 1: send email\n",
    "Q = np.zeros((n_users, max_days_window, n_actions))\n",
    "\n",
    "# Training the Q-learning algorithm\n",
    "learned_optimal_gaps = []\n",
    "\n",
    "for user_id in range(n_users):\n",
    "    user = users.iloc[user_id]\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        days_since_last = 0\n",
    "        n_emails_sent = 0\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Current state\n",
    "            state = (user_id, min(days_since_last, max_days_window-1))\n",
    "            \n",
    "            # Epsilon-greedy action selection\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(0, n_actions)\n",
    "            else:\n",
    "                action = np.argmax(Q[state[0], state[1]])\n",
    "            \n",
    "            # Take action and observe reward\n",
    "            if action == 0:  # Wait\n",
    "                days_since_last += 1\n",
    "                reward = 0\n",
    "                done = days_since_last >= max_days_window\n",
    "            else:  # Send email\n",
    "                response_prob = get_response_probability(user, n_emails_sent, days_since_last)\n",
    "                response = np.random.random() < response_prob\n",
    "                reward = 1 if response else -0.1\n",
    "                n_emails_sent += 1\n",
    "                days_since_last = 0\n",
    "                done = n_emails_sent >= 20  # Limit emails per episode\n",
    "            \n",
    "            # Update Q-value\n",
    "            next_state = (user_id, min(days_since_last, max_days_window-1))\n",
    "            next_max_q = np.max(Q[next_state[0], next_state[1]])\n",
    "            Q[state[0], state[1], action] += learning_rate * (\n",
    "                reward + discount_factor * next_max_q - Q[state[0], state[1], action]\n",
    "            )\n",
    "            \n",
    "            total_reward += reward\n",
    "    \n",
    "    # Find optimal gap for this user\n",
    "    days = 0\n",
    "    max_value = float('-inf')\n",
    "    optimal_gap = 0\n",
    "    \n",
    "    for d in range(max_days_window):\n",
    "        value = Q[user_id, d, 1]  # Value of sending email after d days\n",
    "        if value > max_value:\n",
    "            max_value = value\n",
    "            optimal_gap = d\n",
    "    \n",
    "    learned_optimal_gaps.append(optimal_gap)\n",
    "\n",
    "# Compare learned vs actual optimal gaps\n",
    "results = pd.DataFrame({\n",
    "    'user_id': users['user_id'],\n",
    "    'learned_gap': learned_optimal_gaps,\n",
    "    'actual_gap': users['optimal_gap_days']\n",
    "})\n",
    "\n",
    "print(\"\\nComparison of learned vs actual optimal gaps:\")\n",
    "print(results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of learned vs actual optimal gaps (with improvements):\n",
      "   user_id  learned_gap  actual_gap\n",
      "0        0            3           3\n",
      "1        1            9           7\n",
      "2        2           12          11\n",
      "3        3            4           3\n",
      "4        4            4           5\n",
      "\n",
      "Mean absolute error: 4.39\n"
     ]
    }
   ],
   "source": [
    "# Let's try a few improvements:\n",
    "# 1. Increase training episodes for better convergence\n",
    "# 2. Add epsilon-greedy exploration\n",
    "# 3. Decay learning rate over time\n",
    "# 4. Use a larger penalty for unsuccessful emails\n",
    "\n",
    "n_episodes = 2000  # Increased from previous value\n",
    "epsilon = 0.3  # For exploration\n",
    "learning_rate_start = 0.1\n",
    "learning_rate_end = 0.01\n",
    "min_penalty = -0.5  # Increased penalty for unsuccessful emails\n",
    "\n",
    "learned_optimal_gaps = []\n",
    "\n",
    "for user_id, user in users.iterrows():\n",
    "    Q = np.zeros((len(users), max_days_window, 2))\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = (user_id, 0)  # (user_id, days_since_last)\n",
    "        days_since_last = 0\n",
    "        n_emails_sent = 0\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # Decay learning rate\n",
    "        learning_rate = learning_rate_start - (learning_rate_start - learning_rate_end) * (episode / n_episodes)\n",
    "        \n",
    "        while not done:\n",
    "            # Epsilon-greedy action selection\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.choice([0, 1])\n",
    "            else:\n",
    "                action = np.argmax(Q[state[0], state[1]])\n",
    "            \n",
    "            # Take action and observe reward\n",
    "            if action == 0:  # Wait\n",
    "                days_since_last += 1\n",
    "                reward = 0\n",
    "                done = days_since_last >= max_days_window\n",
    "            else:  # Send email\n",
    "                response_prob = get_response_probability(user, n_emails_sent, days_since_last)\n",
    "                response = np.random.random() < response_prob\n",
    "                reward = 1 if response else min_penalty\n",
    "                n_emails_sent += 1\n",
    "                days_since_last = 0\n",
    "                done = n_emails_sent >= 20\n",
    "            \n",
    "            next_state = (user_id, min(days_since_last, max_days_window-1))\n",
    "            next_max_q = np.max(Q[next_state[0], next_state[1]])\n",
    "            Q[state[0], state[1], action] += learning_rate * (\n",
    "                reward + discount_factor * next_max_q - Q[state[0], state[1], action]\n",
    "            )\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "    \n",
    "    # Find optimal gap for this user\n",
    "    optimal_gap = np.argmax([Q[user_id, d, 1] for d in range(max_days_window)])\n",
    "    learned_optimal_gaps.append(optimal_gap)\n",
    "\n",
    "# Compare results with improved version\n",
    "results = pd.DataFrame({\n",
    "    'user_id': users['user_id'],\n",
    "    'learned_gap': learned_optimal_gaps,\n",
    "    'actual_gap': users['optimal_gap_days']\n",
    "})\n",
    "\n",
    "print(\"\\nComparison of learned vs actual optimal gaps (with improvements):\")\n",
    "print(results.head())\n",
    "print(\"\\nMean absolute error:\", np.mean(np.abs(results['learned_gap'] - results['actual_gap'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
